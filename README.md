# ğŸ“š Persian Information Retrieval System -- Hamshahri Corpus

This project implements a complete **Information Retrieval (IR) system**
for ranking Persian documents using the **Hamshahri Corpus**.

The system includes full preprocessing, indexing, multiple ranking
models, and systematic evaluation across varying retrieval depths.

------------------------------------------------------------------------

## âœ¨ Project Overview

The goal of this project is to design and compare multiple document
ranking models capable of retrieving relevant Persian news articles for
a given query.

The system works on the **Hamshahri Corpus (2003--2007)** and evaluates
performance using official relevance judgments.

------------------------------------------------------------------------

## ğŸ§± Repository Structure

    .
    â”œâ”€â”€ HamshahriCorpus/              # News articles (organized by year)
    â”‚   â”œâ”€â”€ 2003/
    â”‚   â”œâ”€â”€ 2004/
    â”‚   â”œâ”€â”€ 2005/
    â”‚   â”œâ”€â”€ 2006/
    â”‚   â””â”€â”€ 2007/
    â”‚
    â”œâ”€â”€ Queries/                      # 50 test queries (1.q, 2.q, ...)
    â”œâ”€â”€ RelativeAssessment/
    â”‚   â””â”€â”€ judgements.txt            # Ground-truth relevance labels
    â”‚
    â”œâ”€â”€ persian_stopwords.txt         # Persian stopword list
    â”œâ”€â”€ ranking_hamshahri_documents.ipynb # Main implementation notebook
    â””â”€â”€ README.md

    # Note: The "HamshahriCorpus" folder is compressed into a .zip file.

------------------------------------------------------------------------

## ğŸ” Dataset

**Hamshahri Corpus**

-   Persian news collection (2003--2007)
-   Large-scale benchmark dataset for Persian IR research
-   Includes:
    -   50 evaluation queries
    -   Relevance judgments for each query

This dataset presents realistic challenges such as: - Persian text
normalization - Inconsistent punctuation - Noisy tokens - Mixed
character encodings

------------------------------------------------------------------------

## ğŸ§¹ 1. Preprocessing Pipeline

A robust preprocessing pipeline was implemented to improve retrieval
effectiveness:

-   Persian character normalization\
-   Removal of punctuation and unwanted symbols\
-   Removal of English characters\
-   Tokenization\
-   Stopword removal\
-   Term cleaning\
-   Alphabetical sorting of terms\
-   Creation of unique term lists

This ensures high-quality indexing and reduces noise in ranking.

------------------------------------------------------------------------

## ğŸ—‚ 2. Indexing

The following core IR data structures were built:

### ğŸ“Œ Vocabulary

All unique terms across the corpus after preprocessing.

### ğŸ“Œ Inverted Index

Maps each term â†’ list of document IDs containing it.

### ğŸ“Œ Term Statistics

-   **TF (Term Frequency)**
-   **DF (Document Frequency)**
-   **IDF (Inverse Document Frequency)**

IDF formula:

IDF(t) = log(N / DF(t))

These structures enable efficient document scoring.

------------------------------------------------------------------------

## ğŸ“ˆ 3. Ranking Models Implemented

Four ranking models were implemented as independent modules:

### ğŸ”¹ 1. Okapi BM25

-   Probabilistic ranking model\
-   Handles term frequency saturation\
-   Includes document length normalization

### ğŸ”¹ 2. Language Model (LM)

-   Query likelihood approach\
-   Uses smoothing (Dirichlet or Jelinek--Mercer)\
-   Addresses zero-probability issues

### ğŸ”¹ 3. Vector Space Model (VSM)

-   TF-IDF weighting\
-   Cosine similarity\
-   Classic geometric interpretation

### ğŸ”¹ 4. Hybrid Model

-   Weighted score fusion of BM25, LM, and VSM\
-   Improves robustness across query types

------------------------------------------------------------------------

## ğŸ“Š 4. Evaluation

Evaluation was performed using:

RelativeAssessment/judgements.txt

For each query:

-   Let n = number of relevant documents\
-   Retrieve top n Ã— Î± documents\
-   Î± sampled at 20 values in range:

1 â‰¤ Î± â‰¤ 10

------------------------------------------------------------------------

### ğŸ“ Metrics Computed

For each model and each Î±:

-   Precision\
-   Recall\
-   F1-score

Metrics were averaged across all 50 queries.

------------------------------------------------------------------------

### ğŸ“ˆ Visualization

Performance curves were plotted showing:

-   Precision vs Î±\
-   Recall vs Î±\
-   F1-score vs Î±\
-   All four models compared together

This enables clear comparison of trade-offs and model behavior.


------------------------------------------------------------------------

## ğŸ“ Educational Context

Demonstrates:

-   End-to-end IR implementation\
-   Large-scale indexing\
-   Probabilistic & vector ranking\
-   Evaluation under varying retrieval thresholds

------------------------------------------------------------------------

## ğŸš€ Skills Demonstrated

-   Persian text preprocessing\
-   Inverted indexing\
-   TF-IDF modeling\
-   BM25 implementation\
-   Language modeling with smoothing\
-   Hybrid ranking systems\
-   Retrieval evaluation metrics\
-   Experimental analysis